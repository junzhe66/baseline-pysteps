{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jan 17 07:41:32 2019\n",
    "\n",
    "Deterministic nowcast with pySTEPS, with extraction of results per catchment. \n",
    "Based on the input data for the Ensemble nowcast, but without any ensembles. \n",
    "\n",
    "Make sure to change the initial part to your case.\n",
    "\n",
    "Note that this script assumes that the catchments are already reprojected.\n",
    "\n",
    "TO DO - add _reprojected to input and change this later on in the script.\n",
    "\n",
    "@author: imhof_rn\n",
    "\"\"\"\n",
    "\n",
    "from osgeo import gdal\n",
    "from osgeo import gdal_array\n",
    "from osgeo import ogr, osr\n",
    "\n",
    "import os\n",
    "#os.environ['PROJ_LIB'] = r'/u/imhof_rn/anaconda3/pkgs/proj4-5.2.0-h470a237_1/share/proj'\n",
    "os.chdir('/users/junzheyin/Large_Sample_Nowcasting_Evaluation/pysteps')\n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "\n",
    "import datetime\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pysteps as stp\n",
    "import config as cfg\n",
    "\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# import message passing interface for python\n",
    "from mpi4py import MPI\n",
    "\n",
    "# import for memory use\n",
    "#from pympler import tracker\n",
    "#tr = tracker.SummaryTracker()\n",
    "#tr.print_diff() \n",
    "\n",
    "###############################################################################\n",
    "#################\n",
    "# Initial part, only change this\n",
    "# NOTE: This script only works when the catchment shapefiles are already reprojected\n",
    "# to the KNMI radar dataset.\n",
    "#################\n",
    "\n",
    "#os.chdir('/u/imhof_rn/pysteps-0.2')\n",
    "\n",
    "# Catchment filenames and directories\n",
    "catchments = False # Put on false when you don't want any slicing for catchments (i.e. you will use the full output)\n",
    "# If catchments = 'False', uncomment the next two lines.\n",
    "catchment_filenames = [\"/bulk/junzheyin/catchmentss/Hupsel.shp\", \"/bulk/junzheyin/catchments/stroomgebied_Regge.shp\", \"/bulk/junzheyin/catchments/GroteWaterleiding.shp\", \"/bulk/junzheyin/catchments/Aa.shp\", \"/bulk/junzheyin/catchments/Reusel.shp\", \"/bulk/junzheyin/catchments/het_molentje.shp\", \"/bulk/junzheyin/catchments/Luntersebeek.shp\", \"/bulk/junzheyin/catchments/Dwarsdiep.shp\", \"/bulk/junzheyin/catchments/AfwaterendgebiedBoezemsysteem.shp\", \"/bulk/junzheyin/catchments/HHRijnland.shp\", \"/bulk/junzheyin/catchments/Beemster.shp\", \"/bulk/junzheyin/catchments/DeLinde.shp\"] # Put here the locations of the shapefiles\n",
    "catchment_names = ['Hupsel', 'Regge', 'GroteWaterleiding', 'Aa', 'Reusel', 'Molentje', 'Luntersebeek', 'Dwarsdiep', 'Delfland', 'Rijnland', 'Beemster', 'Linde'] # A list of catchment names.\n",
    "out_dir = \"/users/junzheyin/Large_Sample_Nowcasting_Evaluation/pysteps\" # Just used for logging, the actual\n",
    "# out_dir is set in the pystepsrc-file.\n",
    "\n",
    "# Verification settings\n",
    "verification = {\n",
    "    \"experiment_name\"   : \"pysteps_mpi_24hours_deterministic\",\n",
    "    \"overwrite\"         : True,            # to recompute nowcasts\n",
    "    \"v_thresholds\"      : [0.1, 1.0],       # [mm/h]                 \n",
    "    \"v_leadtimes\"       : [10, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360],     # [min]\n",
    "    \"v_accu\"            : None,             # [min]\n",
    "    \"seed\"              : 42,               # for reproducibility\n",
    "    \"doplot\"            : True,            # save figures\n",
    "    \"dosaveresults\"     : True              # save verification scores to csv\n",
    "}\n",
    "\n",
    "# Forecast settings\n",
    "forecast = {\n",
    "    \"n_lead_times\"      : 72,       # timesteps per nowcast\n",
    "    \"r_threshold\"       : 0.1,      # rain/no rain threshold [mm/h]\n",
    "    \"unit\"              : \"mm/h\",   # mm/h or dBZ\n",
    "    \"transformation\"    : \"dB\",     # None or dB \n",
    "    \"adjust_domain\"     : None      # None or square\n",
    "}\n",
    "\n",
    "# The experiment set-up\n",
    "## this includes tuneable parameters\n",
    "experiment = {\n",
    "    ## the events           event start     event end       update cycle  data source\n",
    "    \"data\"              : [('201508131920', '201508132220', 5, 'knmi'), \n",
    "      ('201508132000', '201508132300', 5, 'knmi'), \n",
    "      ('201508261735', '201508262035', 5, 'knmi'), \n",
    "      ('201508261800', '201508262100', 5, 'knmi'), \n",
    "      ('201509041755', '201509042055', 5, 'knmi'), \n",
    "      ('201509041820', '201509042120', 5, 'knmi'), \n",
    "      ('201605301655', '201605301955', 5, 'knmi'), \n",
    "      ('201605301745', '201605302045', 5, 'knmi'), \n",
    "      ('201605301800', '201605302100', 5, 'knmi'), \n",
    "      ('201606201210', '201606201510', 5, 'knmi'), \n",
    "      ('201606201300', '201606201600', 5, 'knmi'), \n",
    "      ('201606222325', '201606230225', 5, 'knmi'), \n",
    "      ('201606230050', '201606230350', 5, 'knmi'), \n",
    "      ('201606230100', '201606230400', 5, 'knmi'), \n",
    "      ('201606230200', '201606230500', 5, 'knmi'), \n",
    "      ('201606230300', '201606230600', 5, 'knmi'), \n",
    "      ('201707061955', '201707062255', 5, 'knmi'), \n",
    "      ('201707062000', '201707062300', 5, 'knmi'), \n",
    "      ('201707120455', '201707120755', 5, 'knmi'), \n",
    "      ('201707120520', '201707120820', 5, 'knmi'), \n",
    "      ('201707120600', '201707120900', 5, 'knmi'), \n",
    "      ('201707120700', '201707121000', 5, 'knmi'), \n",
    "      ('201707291755', '201707292055', 5, 'knmi'), \n",
    "      ('201707291800', '201707292100', 5, 'knmi'), \n",
    "      ('201708300055', '201708300355', 5, 'knmi'), \n",
    "      ('201708300155', '201708300455', 5, 'knmi'), \n",
    "      ('201708300255', '201708300555', 5, 'knmi'), \n",
    "      ('201708300300', '201708300600', 5, 'knmi'), \n",
    "      ('201708300420', '201708300720', 5, 'knmi'), \n",
    "      ('201708300500', '201708300800', 5, 'knmi'), \n",
    "      ('201709081455', '201709081755', 5, 'knmi'), \n",
    "      ('201709081545', '201709081845', 5, 'knmi'), \n",
    "      ('201709081650', '201709081950', 5, 'knmi'), \n",
    "      ('201709081725', '201709082025', 5, 'knmi'), \n",
    "      ('201709081800', '201709082100', 5, 'knmi'), \n",
    "      ('201709081900', '201709082200', 5, 'knmi'), \n",
    "      ('201709141055', '201709141355', 5, 'knmi'), \n",
    "      ('201709141155', '201709141455', 5, 'knmi'), \n",
    "      ('201709141200', '201709141500', 5, 'knmi'), \n",
    "      ('201709141300', '201709141600', 5, 'knmi'), \n",
    "      ('201711270745', '201711271045', 5, 'knmi'), \n",
    "      ('201804102045', '201804102345', 5, 'knmi'), \n",
    "      ('201804102100', '201804110000', 5, 'knmi'), \n",
    "      ('201804102200', '201804110100', 5, 'knmi'), \n",
    "      ('201804292255', '201804300155', 5, 'knmi'), \n",
    "      ('201804292310', '201804300210', 5, 'knmi'), \n",
    "      ('201804300000', '201804300300', 5, 'knmi'), \n",
    "      ('201805291455', '201805291755', 5, 'knmi'), \n",
    "      ('201805291515', '201805291815', 5, 'knmi'), \n",
    "      ('201805291600', '201805291900', 5, 'knmi'), \n",
    "      ('201808101755', '201808102055', 5, 'knmi'), \n",
    "      ('201808101820', '201808102120', 5, 'knmi'), \n",
    "      ('201808101930', '201808102230', 5, 'knmi'), \n",
    "      ('201808102000', '201808102300', 5, 'knmi'), \n",
    "      ('201808242050', '201808242350', 5, 'knmi'), \n",
    "      ('201808242100', '201808250000', 5, 'knmi'), \n",
    "      ('201808242240', '201808250140', 5, 'knmi'), \n",
    "      ('201808242300', '201808250200', 5, 'knmi'), \n",
    "      ('201809050545', '201809050845', 5, 'knmi'), \n",
    "      ('201809050600', '201809050900', 5, 'knmi'), \n",
    "      ('201809050700', '201809051000', 5, 'knmi'), \n",
    "      ('201810300150', '201810300450', 5, 'knmi'), \n",
    "      ('201810300200', '201810300500', 5, 'knmi'), \n",
    "      ('201810300300', '201810300600', 5, 'knmi'), \n",
    "      ('201906052100', '201906060000', 5, 'knmi'), \n",
    "      ('201906052200', '201906060100', 5, 'knmi'), \n",
    "      ('201906052300', '201906060200', 5, 'knmi'), \n",
    "      ('201906120755', '201906121055', 5, 'knmi'), \n",
    "      ('201906120820', '201906121120', 5, 'knmi'), \n",
    "      ('201906120940', '201906121240', 5, 'knmi'), \n",
    "      ('201906121000', '201906121300', 5, 'knmi'), \n",
    "      ('201906150155', '201906150455', 5, 'knmi'), \n",
    "      ('201906150240', '201906150540', 5, 'knmi'), \n",
    "      ('201906150300', '201906150600', 5, 'knmi'), \n",
    "      ('201906150400', '201906150700', 5, 'knmi'), \n",
    "      ('201910061250', '201910061550', 5, 'knmi'), \n",
    "      ('201910210430', '201910210730', 5, 'knmi'), \n",
    "      ('202002091830', '202002092130', 5, 'knmi'), \n",
    "      ('202002091900', '202002092200', 5, 'knmi'), \n",
    "      ('202006050545', '202006050845', 5, 'knmi'), \n",
    "      ('202006050600', '202006050900', 5, 'knmi'), \n",
    "      ('202006122025', '202006122325', 5, 'knmi'), \n",
    "      ('202006171755', '202006172055', 5, 'knmi'), \n",
    "      ('202006171855', '202006172155', 5, 'knmi'), \n",
    "      ('202006171920', '202006172220', 5, 'knmi'), \n",
    "      ('202006172000', '202006172300', 5, 'knmi'), \n",
    "      ('202007251955', '202007252255', 5, 'knmi'), \n",
    "      ('202007252055', '202007252355', 5, 'knmi'), \n",
    "      ('202007252100', '202007260000', 5, 'knmi'), \n",
    "      ('202008161555', '202008161855', 5, 'knmi'), \n",
    "      ('202008161600', '202008161900', 5, 'knmi'), \n",
    "      ('202009232020', '202009232320', 5, 'knmi'), \n",
    "      ('202009232100', '202009240000', 5, 'knmi')],\n",
    "     \n",
    "                                                                                                                                                \n",
    "    ## the methods\n",
    "    \"oflow_method\"      : [\"lucaskanade\"],      # lucaskanade, darts\n",
    "    \"adv_method\"        : [\"semilagrangian\"],   # semilagrangian, eulerian\n",
    "    \"nwc_method\"        : [\"steps\"],\n",
    "    \"noise_method\"      : [None],    # parametric, nonparametric, ssft\n",
    "    \"decomp_method\"     : [\"fft\"],\n",
    "    \n",
    "    ## the parameters\n",
    "    \"n_ens_members\"     : [1],\n",
    "    \"ar_order\"          : [2],\n",
    "    \"n_cascade_levels\"  : [8],\n",
    "    \"noise_adjustment\"  : [False],\n",
    "    \"conditional\"       : [False],\n",
    "    \"precip_mask\"       : [True],\n",
    "    \"mask_method\"       : [\"sprog\"],      # obs, incremental, sprog\n",
    "    \"prob_matching\"     : [\"mean\"],\n",
    "    \"num_workers\"       : [1],         # Set the number of processors available for parallel computing\n",
    "    \"vel_pert_method\"   : [None],       # No velocity pertubation in order to allow for deterministic run following Seed et al. [2003]\n",
    "}\n",
    "\n",
    "# End of initial part\n",
    "###############################################################################\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#### HERE ALL AVAILABLE PROCESSES AT START-UP TIME ARE COLLECTED IN comm\n",
    "#### SEE FOR MORE INFO ON MPI: https://www.cs.earlham.edu/~lemanal/slides/mpi-slides.pdf \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "\n",
    "logging.info(('I am process rank {}'.format(rank)))\n",
    "\n",
    "#########################################################\n",
    "# Open the catchment shapes - They're needed later for the catchment_slice utils\n",
    "#########################################################\n",
    "shapes = []\n",
    "\n",
    "for i in range(0, len(catchment_filenames)):\n",
    "    shape_filename = catchment_filenames[i]\n",
    "    \n",
    "    # set file names in order to obtain the reprojected shapefile, which \n",
    "    # was made with the catchment_medata functionality.\n",
    "    dirname = os.path.dirname(shape_filename)\n",
    "    basename = os.path.basename(shape_filename)\n",
    "    basenametxt = os.path.splitext(basename)[0]\n",
    "    shapes_reprojected = os.path.join(dirname, basenametxt+'_Reprojected.shp')\t\n",
    "    \n",
    "    driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    shapes.append(driver.Open(shapes_reprojected))\n",
    "\n",
    "###########\n",
    "# Set some first functions\n",
    "###########\n",
    "\n",
    "## define the callback function to export the nowcast to netcdf\n",
    "converter   = stp.utils.get_method(\"mm/h\")\n",
    "def export(X):\n",
    "    ## convert to mm/h\n",
    "    X,_ = converter(X, metadata)\n",
    "    # readjust to initial domain shape\n",
    "    X,_ = reshaper(X, metadata, inverse=True)\n",
    "    # Then, slice the array per catchment or not if no catchments are given\n",
    "    if catchments == True:\n",
    "        X_catchment = stp.utils.catchment_slice_mpi(X, shapes)\n",
    "        # Export to netCDF per catchment\n",
    "        for n in range(0, len(catchment_filenames)):\n",
    "            key = list(d.keys())[n]\n",
    "            stp.io.export_forecast_dataset(X_catchment[n], d[key])\n",
    "    else:\n",
    "        # else, export full radar nowcast to netcdf\n",
    "        stp.io.export_forecast_dataset(X, exporter)\n",
    "\n",
    "# Conditional parameters\n",
    "## parameters that can be directly related to other parameters\n",
    "def cond_pars(pars):\n",
    "    for key in list(pars):\n",
    "        if key == \"oflow_method\":\n",
    "            if pars[key].lower() == \"darts\":  pars[\"n_prvs_times\"] = 9\n",
    "            else:                             pars[\"n_prvs_times\"] = 3\n",
    "        elif key.lower() == \"n_cascade_levels\":\n",
    "            if pars[key] == 1 : pars[\"bandpass_filter\"] = \"uniform\"\n",
    "            else:               pars[\"bandpass_filter\"] = \"gaussian\"\n",
    "        elif key.lower() == \"nwc_method\":\n",
    "            if pars[key] == \"extrapolation\" : pars[\"n_ens_members\"] = 1\n",
    "    return pars\n",
    "\n",
    "#########\n",
    "# Make list of parameters (i.e. the different dates - all other parameters are\n",
    "# the same for every run) and scatter these over the nodes.\n",
    "#########\n",
    "    \n",
    "# Prepare the list of all parameter sets of the verification\n",
    "parsets = [[]]\n",
    "for _, items in experiment.items():\n",
    "    parsets = [parset+[item] for parset in parsets for item in items]\n",
    "\n",
    "if rank == 0:\n",
    "    #### Reorganize work a bit so we can scatter it\n",
    "    keyfunc = lambda x:x[0] % size\n",
    "    work = itertools.groupby(sorted(enumerate(parsets), key=keyfunc), keyfunc)\n",
    "    \n",
    "    #### Expand the work so we get lists of row, col per node\n",
    "    workpernode = [[x[1] for x in val] for (key, val) in work]\n",
    "else:\n",
    "    workpernode = None\n",
    "\n",
    "#### NOW DISTRIBUTE THE WORK\n",
    "workpernode = comm.scatter(workpernode, root=0)\n",
    "\n",
    "logging.info(\"Got the following work in process rank {} : {}\".format(rank, workpernode))\n",
    "\n",
    "#### Each node can now do it's own work. The main advantage is that we can do a gather at the end to collect all results.\n",
    "#### Keep track of all the runs per node in scores\n",
    "#scores = []\n",
    "\n",
    "#### before starting any runs, make sure that you know in which folder we run this MPI run routine. \n",
    "#### Always return to this folder before the next run\n",
    "#curdir = os.getcwd()\n",
    "#os.chdir('/u/imhof_rn/pysteps-master')\n",
    "\n",
    "###########\n",
    "# Run the model in parallel\n",
    "###########\n",
    "\n",
    "# Now loop all parameter sets\n",
    "for n, parset in enumerate(workpernode):\n",
    "#    logging.info(\"rank %02.f computing scores for parameter set nr %04.f\" % (rank, n))\n",
    "    runId = '%s_%04.f' % (out_dir, n)\n",
    "    \n",
    "    # Build parameter set\n",
    "    \n",
    "    p = {}\n",
    "    for m, key in enumerate(experiment.keys()):\n",
    "        p[key] = parset[m]\n",
    "    ## apply conditional parameters\n",
    "    p = cond_pars(p)\n",
    "    ## include all remaining parameters\n",
    "    p.update(verification)\n",
    "    p.update(forecast)\n",
    "    \n",
    "#    print(\"************************\")\n",
    "#    print(\"* Parameter set %02d/%02d: *\" % (n+1, len(parsets)))\n",
    "#    print(\"************************\")\n",
    "    \n",
    "#    pprint.pprint(p)\n",
    "    \n",
    "    # If necessary, build path to results\n",
    "    path_to_experiment = os.path.join(cfg.path_outputs, p[\"experiment_name\"])\n",
    "    # subdir with event date\n",
    "    path_to_nwc = os.path.join(path_to_experiment, '-'.join([p[\"data\"][0], p[\"data\"][3]]))\n",
    "#    for key, item in p.items():\n",
    "#\t\t# include only variables that change\n",
    "#        if len(experiment.get(key,[None])) > 1 and key.lower() is not \"data\":\n",
    "#            path_to_nwc = os.path.join(path_to_nwc, '-'.join([key, str(item)]))\n",
    "    try:\n",
    "        os.makedirs(path_to_nwc)\n",
    "    except OSError:\n",
    "        pass\n",
    "        \n",
    "    # **************************************************************************\n",
    "    # NOWCASTING\n",
    "    # ************************************************************************** \n",
    "    \n",
    "    # Loop forecasts within given event using the prescribed update cycle interval\n",
    "\n",
    "    ## import data specifications\n",
    "    ds = cfg.get_specifications(p[\"data\"][3])\n",
    "    \n",
    "    if p[\"v_accu\"] is None:\n",
    "        p[\"v_accu\"] = ds.timestep\n",
    "    \n",
    "    # Loop forecasts for given event\n",
    "    startdate   = datetime.datetime.strptime(p[\"data\"][0], \"%Y%m%d%H%M\")\n",
    "    enddate     = datetime.datetime.strptime(p[\"data\"][1], \"%Y%m%d%H%M\")\n",
    "    countnwc = 0\n",
    "    while startdate <= enddate:\n",
    "            # filename of the nowcast netcdf. Set name either per catchment or as \n",
    "            # total nowcast for the entire radar image.\n",
    "            if catchments == True:\n",
    "                outfn = []\n",
    "                for n in range(0, len(catchment_names)):\n",
    "                    path_to_catchment = os.path.join(path_to_nwc, catchment_names[n])\n",
    "                    try:\n",
    "                        os.makedirs(path_to_catchment)\n",
    "                        Name = os.path.join(path_to_catchment, \"%s_nowcast.netcdf\" % startdate.strftime(\"%Y%m%d%H%M\"))\n",
    "                        outfn.append(Name)\n",
    "                    except OSError:\n",
    "                        print(\"Catchment outfile directory does already exist for starttime: %s\" % startdate.strftime(\"%Y%m%d%H%M\"))\n",
    "                        Name = os.path.join(path_to_catchment, \"%s_nowcast.netcdf\" % startdate.strftime(\"%Y%m%d%H%M\"))\n",
    "                        outfn.append(Name)\n",
    "            else:\n",
    "                outfn = os.path.join(path_to_nwc, \"%s_nowcast.netcdf\" % startdate.strftime(\"%Y%m%d%H%M\"))\n",
    "        \n",
    "            ## check if results already exists\n",
    "            if catchments == True:\n",
    "                run_exist = False\n",
    "                if os.path.isfile(outfn[n]):\n",
    "                    fid = netCDF4.Dataset(outfn[n], 'r')\n",
    "                    if fid.dimensions[\"time\"].size == p[\"n_lead_times\"]:\n",
    "                        run_exist = True\n",
    "                        if p[\"overwrite\"]:\n",
    "                            os.remove(outfn[n])\n",
    "                            run_exist = False    \n",
    "                    else:\n",
    "                        os.remove(outfn[n])\n",
    "            else:\n",
    "                run_exist = False\n",
    "                if os.path.isfile(outfn):\n",
    "                    fid = netCDF4.Dataset(outfn, 'r')\n",
    "                    if fid.dimensions[\"time\"].size == p[\"n_lead_times\"]:\n",
    "                        run_exist = True\n",
    "                        if p[\"overwrite\"]:\n",
    "                            os.remove(outfn)\n",
    "                            run_exist = False    \n",
    "                    else:\n",
    "                        os.remove(outfn)\n",
    "                    \n",
    "            if run_exist:\n",
    "                print(\"Nowcast %s_nowcast already exists in %s\" % (startdate.strftime(\"%Y%m%d%H%M\"),path_to_nwc))\n",
    "    \n",
    "            else:\n",
    "                countnwc += 1\n",
    "                print(\"Computing the nowcast (%02d) ...\" % countnwc)\n",
    "                \n",
    "                print(\"Starttime: %s\" % startdate.strftime(\"%Y%m%d%H%M\"))\n",
    "                \n",
    "                ## redirect stdout to log file\n",
    "                logfn =  os.path.join(path_to_nwc, \"%s_log.txt\" % startdate.strftime(\"%Y%m%d%H%M\")) \n",
    "                print(\"Log: %s\" % logfn)\n",
    "                orig_stdout = sys.stdout\n",
    "                f = open(logfn, 'w')\n",
    "                sys.stdout = f\n",
    "                \n",
    "                print(\"*******************\")\n",
    "                print(\"* %s *****\" % startdate.strftime(\"%Y%m%d%H%M\"))\n",
    "                print(\"* Parameter set : *\")\n",
    "    #            pprint.pprint(p) \n",
    "                print(\"*******************\")\n",
    "                \n",
    "                print(\"--- Start of the run : %s ---\" % (datetime.datetime.now()))\n",
    "                \n",
    "                ## time\n",
    "                t0 = time.time()\n",
    "            \n",
    "                # Read inputs\n",
    "    #            print(\"Read the data...\")\n",
    "                \n",
    "                ## find radar field filenames\n",
    "                input_files = stp.io.find_by_date(startdate, ds.root_path, ds.path_fmt, ds.fn_pattern,\n",
    "                                                  ds.fn_ext, ds.timestep, p[\"n_prvs_times\"])\n",
    "                \n",
    "        \n",
    "                ## read radar field files\n",
    "                importer    = stp.io.get_method(ds.importer, type=\"importer\")\n",
    "                R, _, metadata = stp.io.read_timeseries(input_files, importer, **ds.importer_kwargs)\n",
    "                metadata0 = metadata.copy()\n",
    "                metadata0[\"shape\"] = R.shape[1:]\n",
    "                \n",
    "                # Prepare input files\n",
    "    #            print(\"Prepare the data...\")\n",
    "                \n",
    "                ## if requested, make sure we work with a square domain\n",
    "                reshaper = stp.utils.get_method(p[\"adjust_domain\"])\n",
    "                R, metadata = reshaper(R, metadata)\n",
    "        \n",
    "                ## if necessary, convert to rain rates [mm/h]    \n",
    "                converter = stp.utils.get_method(\"mm/h\")\n",
    "                R, metadata = converter(R, metadata)\n",
    "                \n",
    "                ## threshold the data\n",
    "                R[R < p[\"r_threshold\"]] = 0.0\n",
    "                metadata[\"threshold\"] = p[\"r_threshold\"]\n",
    "                \n",
    "                ## convert the data\n",
    "                converter = stp.utils.get_method(p[\"unit\"])\n",
    "                R, metadata = converter(R, metadata)\n",
    "                    \n",
    "                ## transform the data\n",
    "                transformer = stp.utils.get_method(p[\"transformation\"])\n",
    "                R, metadata = transformer(R, metadata)\n",
    "                \n",
    "                ## set NaN equal to zero\n",
    "                R[~np.isfinite(R)] = metadata[\"zerovalue\"]\n",
    "                \n",
    "                # Compute motion field\n",
    "                oflow_method = stp.motion.get_method(p[\"oflow_method\"])\n",
    "                UV = oflow_method(R)\n",
    "                \n",
    "                #####\n",
    "                # Perform the nowcast       \n",
    "                #####\n",
    "                \n",
    "                ## initialize netcdf file\n",
    "                incremental = \"timestep\" if p[\"nwc_method\"].lower() == \"steps\" else None\n",
    "                if catchments == True:\n",
    "                    metadata_new = stp.utils.catchment_metadata_mpi(shapes, metadata0)\n",
    "                    d = {}       \n",
    "                    for n in range(0, len(catchment_filenames)):\n",
    "                        d[\"exporter_{0}\".format(n)] = stp.io.initialize_forecast_exporter_netcdf(outfn[n], startdate,\n",
    "                                                      ds.timestep, p[\"n_lead_times\"], metadata_new[n][\"shape\"], \n",
    "                                                      p[\"n_ens_members\"], metadata_new[n], incremental=incremental)\n",
    "                else:\n",
    "                    exporter = stp.io.initialize_forecast_exporter_netcdf(outfn, startdate,\n",
    "                                  ds.timestep, p[\"n_lead_times\"], metadata0[\"shape\"], \n",
    "                                  p[\"n_ens_members\"], metadata0, incremental=incremental)\n",
    "                \n",
    "                ## start the nowcast\n",
    "                nwc_method = stp.nowcasts.get_method(p[\"nwc_method\"])\n",
    "                R_fct = nwc_method(R, UV, p[\"n_lead_times\"], p[\"n_ens_members\"],\n",
    "                                p[\"n_cascade_levels\"], kmperpixel=metadata[\"xpixelsize\"]/1000, \n",
    "                                timestep=ds.timestep, R_thr=metadata[\"threshold\"], \n",
    "                                extrap_method=p[\"adv_method\"], \n",
    "                                decomp_method=p[\"decomp_method\"], \n",
    "                                bandpass_filter_method=p[\"bandpass_filter\"], \n",
    "                                noise_method=p[\"noise_method\"], \n",
    "                                noise_stddev_adj=p[\"noise_adjustment\"],\n",
    "                                vel_pert_method=p[\"vel_pert_method\"],\n",
    "                                ar_order=p[\"ar_order\"],conditional=p[\"conditional\"], \n",
    "                                probmatching_method=p[\"prob_matching\"], \n",
    "                                mask_method=p[\"mask_method\"], \n",
    "                                num_workers=p[\"num_workers\"],\n",
    "                                callback=export, \n",
    "                                return_output=False)\n",
    "                \n",
    "                ## save results, either per catchment or in total\n",
    "                if catchments == True:\n",
    "                    for n in range(0, len(catchment_filenames)):\n",
    "                        key = list(d.keys())[n]\n",
    "                        stp.io.close_forecast_file(d[key])\n",
    "                else:\n",
    "                    stp.io.close_forecast_file(exporter)\n",
    "                R_fct = None\n",
    "                \n",
    "                # save log\n",
    "                print(\"--- End of the run : %s ---\" % (datetime.datetime.now()))\n",
    "                print(\"--- Total time : %s seconds ---\" % (time.time() - t0))\n",
    "                sys.stdout = orig_stdout\n",
    "                f.close()\n",
    "                \n",
    "            # next forecast\n",
    "            startdate += datetime.timedelta(minutes = p[\"data\"][2])\n",
    "\n",
    "\n",
    "\n",
    "#    tr.print_diff()\n",
    "#    scores.append(n)\n",
    "    #### RETURN TO THE CORRECT DIRECTORY, JUST IN CASE SOMETHING WAS CHANGED...\n",
    "  #  os.chdir('/u/imhof_rn/pysteps-master')\n",
    "\n",
    "#### Wait here so we can collect all runs\n",
    "#### Because we distributed the work evenly all processes should be here at approximately the same time\n",
    "comm.Barrier()\n",
    "#### Great, we're all here. Now let's gather the scores...\n",
    "#### Collect values from all the processes in the main root\n",
    "#scores = comm.gather(scores, root=0)\n",
    "\n",
    "#logging.debug(\"Rank {} has scores {}\".format(rank, scores))\n",
    "  \n",
    "end_time = time.time()\n",
    "\n",
    "print('Total process took', (end_time - start_time)/3600.0, 'hours')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
